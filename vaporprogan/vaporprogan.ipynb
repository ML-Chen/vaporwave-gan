{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VaporProGAN\n",
    "\n",
    "Modified from Fabian Nguyen's code at https://www.kaggle.com/fnguyen/vaporprogan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data\n",
    "\n",
    "Originally, the training data was sourced from r/VaporwaveArt subreddit and provides us with 909 images that have been resized to 128x128 pixel images and converted to arrays using PIL. All 909 images were converted into one 909x128x128x3 Numpy array.\n",
    "\n",
    "Our newly scraped set of ~15,000 images is 256x256, so we will process those into a Numpy array.\n",
    "\n",
    "We will have to modify the code to work with 256x256 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from DomTomCat's answer at https://stackoverflow.com/a/37747559, CC-BY-SA 4.0\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "X_data = []\n",
    "files = glob.glob(r\"D:\\Dropbox (GaTech)\\vaporwave\\256x256png\\*.png\")\n",
    "for myFile in files:\n",
    "    assert image.shape == (256, 256, 3), \"img %s has shape %r\" % (myFile, image.shape)\n",
    "    print(myFile)\n",
    "    image = cv2.imread (myFile)\n",
    "    X_data.append (image)\n",
    "\n",
    "print('X_data shape:', np.array(X_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('vaporwave', X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Add\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import backend\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as pyplot\n",
    "import sys\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples():\n",
    "\t# load Vaporarray dataset\n",
    "\tX = np.load('./test.out.npy')\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(909, 128, 128, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_real_samples().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example of defining composite models for the progressive growing gan\n",
    "# weighted sum output\n",
    "class WeightedSum(Add):\n",
    "\t# init with default value\n",
    "\tdef __init__(self, alpha=0.0, **kwargs):\n",
    "\t\tsuper(WeightedSum, self).__init__(**kwargs)\n",
    "\t\tself.alpha = backend.variable(alpha, name='ws_alpha')\n",
    "\n",
    "\t# output a weighted sum of inputs\n",
    "\tdef _merge_function(self, inputs):\n",
    "\t\t# only supports a weighted sum of two inputs\n",
    "\t\tassert (len(inputs) == 2)\n",
    "\t\t# ((1-a) * input1) + (a * input2)\n",
    "\t\toutput = ((1.0 - self.alpha) * inputs[0]) + (self.alpha * inputs[1])\n",
    "\t\treturn output\n",
    "\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# choose random instances\n",
    "\tix = randint(0, dataset.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX = dataset[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = np.random.uniform(low=0.8, high=1, size=(n_samples,1))\n",
    "\treturn X, y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim: int, n_samples: int) -> np.array:\n",
    "\t# generate points in the latent space\n",
    "\tx_input = randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim: int, n_samples: int) -> Tuple[np.array, np.array]:\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\tX = g_model.predict(x_input)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = zeros((n_samples, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# create and save a plot of generated images\n",
    "def save_plot(examples: np.array, epoch: int, n=7):\n",
    "\t# plot images\n",
    "\texamples\n",
    "\tfor i in range(n * n):\n",
    "\t\t# define subplot\n",
    "\t\tpyplot.subplot(n, n, 1 + i)\n",
    "\t\t# turn off axis\n",
    "\t\tpyplot.axis('off')\n",
    "\t\t# plot raw pixel data\n",
    "\t\tpyplot.imshow(examples[i])\n",
    "\t# save plot to file\n",
    "\tfilename = 'vaporwave_e%03d.png' % (epoch+1)\n",
    "\tpyplot.savefig(filename)\n",
    "\tpyplot.close()\n",
    "    \n",
    "# evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):\n",
    "\t# prepare fake examples\n",
    "\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "\t# save plot\n",
    "\tsave_plot(x_fake, epoch)\n",
    "\t# save the generator model tile file\n",
    "\tfilename = 'generator_model_%03d.h5' % (epoch+1)\n",
    "\tg_model.save(filename)\n",
    "\n",
    "# add a discriminator block\n",
    "def add_discriminator_block(old_model, n_input_layers=3):\n",
    "\t# get shape of existing model\n",
    "\tin_shape = list(old_model.input.shape)\n",
    "\t# define new input shape as double the size\n",
    "\tinput_shape = (in_shape[-2].value*2, in_shape[-2].value*2, in_shape[-1].value)\n",
    "\tin_image = Input(shape=input_shape)\n",
    "\t# define new input processing layer\n",
    "\td = Conv2D(64, (1,1), padding='same', kernel_initializer='he_normal')(in_image)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# define new block\n",
    "\td = Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal')(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\td = Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal')(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\td = AveragePooling2D()(d)\n",
    "\tblock_new = d\n",
    "\t# skip the input, 1x1 and activation for the old model\n",
    "\tfor i in range(n_input_layers, len(old_model.layers)):\n",
    "\t\td = old_model.layers[i](d)\n",
    "\t# define straight-through model\n",
    "\tmodel1 = Model(in_image, d)\n",
    "\t# compile model\n",
    "\tmodel1.compile(loss='mse', optimizer=Adam(lr=0.0004, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\t# downsample the new larger image\n",
    "\tdownsample = AveragePooling2D()(in_image)\n",
    "\t# connect old input processing to downsampled new input\n",
    "\tblock_old = old_model.layers[1](downsample)\n",
    "\tblock_old = old_model.layers[2](block_old)\n",
    "\t# fade in output of old model input layer with new input\n",
    "\td = WeightedSum()([block_old, block_new])\n",
    "\t# skip the input, 1x1 and activation for the old model\n",
    "\tfor i in range(n_input_layers, len(old_model.layers)):\n",
    "\t\td = old_model.layers[i](d)\n",
    "\t# define straight-through model\n",
    "\tmodel2 = Model(in_image, d)\n",
    "\t# compile model\n",
    "\tmodel2.compile(loss='mse', optimizer=Adam(lr=0.0004, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\treturn [model1, model2]\n",
    "\n",
    "# define the discriminator models for each image resolution\n",
    "def define_discriminator(n_blocks, input_shape=(4,4,3)):\n",
    "\tmodel_list = list()\n",
    "\t# base model input\n",
    "\tin_image = Input(shape=input_shape)\n",
    "\t# conv 1x1\n",
    "\td = Conv2D(64, (1,1), padding='same', kernel_initializer='he_normal')(in_image)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# conv 3x3 (output block)\n",
    "\td = Conv2D(128, (3,3), padding='same', kernel_initializer='he_normal')(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# conv 4x4\n",
    "\td = Conv2D(128, (4,4), padding='same', kernel_initializer='he_normal')(d)\n",
    "\td = BatchNormalization()(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# dense output layer\n",
    "\td = Flatten()(d)\n",
    "\tout_class = Dense(1)(d)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, out_class)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='mse', optimizer=Adam(lr=0.0004, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\t# store model\n",
    "\tmodel_list.append([model, model])\n",
    "\t# create submodels\n",
    "\tfor i in range(1, n_blocks):\n",
    "\t\t# get prior model without the fade-on\n",
    "\t\told_model = model_list[i - 1][0]\n",
    "\t\t# create new model for next resolution\n",
    "\t\tmodels = add_discriminator_block(old_model)\n",
    "\t\t# store model\n",
    "\t\tmodel_list.append(models)\n",
    "\treturn model_list\n",
    "\n",
    "# add a generator block\n",
    "def add_generator_block(old_model):\n",
    "\t# get the end of the last block\n",
    "\tblock_end = old_model.layers[-2].output\n",
    "\t# upsample, and define new block\n",
    "\tupsampling = UpSampling2D()(block_end)\n",
    "\tg = Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal')(upsampling)\n",
    "\tg = BatchNormalization()(g)\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\tg = Conv2D(64, (3,3), padding='same', kernel_initializer='he_normal')(g)\n",
    "\tg = BatchNormalization()(g)\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\t# add new output layer\n",
    "\tout_image = Conv2D(3, (1,1), padding='same', kernel_initializer='he_normal')(g)\n",
    "\t# define model\n",
    "\tmodel1 = Model(old_model.input, out_image)\n",
    "\t# get the output layer from old model\n",
    "\tout_old = old_model.layers[-1]\n",
    "\t# connect the upsampling to the old output layer\n",
    "\tout_image2 = out_old(upsampling)\n",
    "\t# define new output image as the weighted sum of the old and new models\n",
    "\tmerged = WeightedSum()([out_image2, out_image])\n",
    "\t# define model\n",
    "\tmodel2 = Model(old_model.input, merged)\n",
    "\treturn [model1, model2]\n",
    "\n",
    "# define generator models\n",
    "def define_generator(latent_dim, n_blocks, in_dim=4):\n",
    "\tmodel_list = list()\n",
    "\t# base model latent input\n",
    "\tin_latent = Input(shape=(latent_dim,))\n",
    "\t# linear scale up to activation maps\n",
    "\tg  = Dense(128 * in_dim * in_dim, kernel_initializer='he_normal')(in_latent)\n",
    "\tg = Reshape((in_dim, in_dim, 128))(g)\n",
    "\t# conv 4x4, input block\n",
    "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer='he_normal')(g)\n",
    "\tg = BatchNormalization()(g)\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\t# conv 3x3\n",
    "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer='he_normal')(g)\n",
    "\tg = BatchNormalization()(g)\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\t# conv 1x1, output block\n",
    "\tout_image = Conv2D(3, (1,1), padding='same', kernel_initializer='he_normal', activation = 'sigmoid')(g)\n",
    "\t# define model\n",
    "\tmodel = Model(in_latent, out_image)\n",
    "\t# store model\n",
    "\tmodel_list.append([model, model])\n",
    "\t# create submodels\n",
    "\tfor i in range(1, n_blocks):\n",
    "\t\t# get prior model without the fade-on\n",
    "\t\told_model = model_list[i - 1][0]\n",
    "\t\t# create new model for next resolution\n",
    "\t\tmodels = add_generator_block(old_model)\n",
    "\t\t# store model\n",
    "\t\tmodel_list.append(models)\n",
    "\treturn model_list\n",
    "\n",
    "# define composite models for training generators via discriminators\n",
    "def define_composite(discriminators, generators):\n",
    "\tmodel_list = list()\n",
    "\t# create composite models\n",
    "\tfor i in range(len(discriminators)):\n",
    "\t\tg_models, d_models = generators[i], discriminators[i]\n",
    "\t\t# straight-through model\n",
    "\t\td_models[0].trainable = False\n",
    "\t\tmodel1 = Sequential()\n",
    "\t\tmodel1.add(g_models[0])\n",
    "\t\tmodel1.add(d_models[0])\n",
    "\t\tmodel1.compile(loss='mse', optimizer=Adam(lr=0.0001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\t\t# fade-in model\n",
    "\t\td_models[1].trainable = False\n",
    "\t\tmodel2 = Sequential()\n",
    "\t\tmodel2.add(g_models[1])\n",
    "\t\tmodel2.add(d_models[1])\n",
    "\t\tmodel2.compile(loss='mse', optimizer=Adam(lr=0.0001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\t\t# store\n",
    "\t\tmodel_list.append([model1, model2])\n",
    "\treturn model_list\n",
    "\n",
    "# define models\n",
    "discriminators = define_discriminator(6)\n",
    "# define models\n",
    "generators = define_generator(100, 6)\n",
    "# define composite models\n",
    "composite = define_composite(discriminators, generators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fade-in/-out**\n",
    "\n",
    "This is a crucially part of building the model, where the different layers of the progression are merged into one training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the alpha value on each instance of WeightedSum\n",
    "def update_fadein(models, step, n_steps):\n",
    "\t# calculate current alpha (linear from 0 to 1)\n",
    "\talpha = step / float(n_steps - 1)\n",
    "\t# update the alpha for each model\n",
    "\tfor model in models:\n",
    "\t\tfor layer in model.layers:\n",
    "\t\t\tif isinstance(layer, WeightedSum):\n",
    "\t\t\t\tbackend.set_value(layer.alpha, alpha)\n",
    "                \n",
    "# train a generator and discriminator\n",
    "def train_epochs(g_model, d_model, gan_model, dataset, n_epochs, n_batch, fadein=False):\n",
    "\t# calculate the number of batches per training epoch\n",
    "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "\t# calculate the number of training iterations\n",
    "\tn_steps = bat_per_epo * n_epochs\n",
    "\t# calculate the size of half a batch of samples\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_steps):\n",
    "\t\t# update alpha for all WeightedSum layers when fading in new blocks\n",
    "\t\tif fadein:\n",
    "\t\t\tupdate_fadein([g_model, d_model, gan_model], i, n_steps)\n",
    "\t\t# prepare real and fake samples\n",
    "\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t# update discriminator model\n",
    "\t\td_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "\t\td_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\tz_input = generate_latent_points(latent_dim, n_batch)\n",
    "\t\ty_real2 = ones((n_batch, 1))\n",
    "\t\tg_loss = gan_model.train_on_batch(z_input, y_real2)\n",
    "        \n",
    "# scale images to preferred size\n",
    "def scale_dataset(images, new_shape):\n",
    "\timages_list = list()\n",
    "\tfor image in images:\n",
    "\t\t# resize with nearest neighbor interpolation\n",
    "\t\tnew_image = resize(image, new_shape, 0)\n",
    "\t\t# store\n",
    "\t\timages_list.append(new_image)\n",
    "\treturn np.asarray(images_list)\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_models, d_models, gan_models, dataset, latent_dim, e_norm, e_fadein, n_batch):\n",
    "\t# fit the baseline model\n",
    "\tg_normal, d_normal, gan_normal = g_models[0][0], d_models[0][0], gan_models[0][0]\n",
    "\t# scale dataset to appropriate size\n",
    "\tgen_shape = g_normal.output_shape\n",
    "\tscaled_data = scale_dataset(dataset, gen_shape[1:])\n",
    "\tprint('Scaled Data', scaled_data.shape)\n",
    "\t# train normal or straight-through models\n",
    "\ttrain_epochs(g_normal, d_normal, gan_normal, scaled_data, e_norm, n_batch)\n",
    "\t# process each level of growth\n",
    "\tfor i in range(1, len(g_models)):\n",
    "\t\t# retrieve models for this level of growth\n",
    "\t\t[g_normal, g_fadein] = g_models[i]\n",
    "\t\t[d_normal, d_fadein] = d_models[i]\n",
    "\t\t[gan_normal, gan_fadein] = gan_models[i]\n",
    "\t\t# scale dataset to appropriate size\n",
    "\t\tgen_shape = g_normal.output_shape\n",
    "\t\tscaled_data = scale_dataset(dataset, gen_shape[1:])\n",
    "\t\tprint('Scaled Data', scaled_data.shape)\n",
    "\t\t# train fade-in models for next level of growth\n",
    "\t\ttrain_epochs(g_fadein, d_fadein, gan_fadein, scaled_data, e_fadein, n_batch, True)\n",
    "\t\t# train normal or straight-through models\n",
    "\t\ttrain_epochs(g_normal, d_normal, gan_normal, scaled_data, e_norm, n_batch)\n",
    "\t\tsummarize_performance(i, g_normal, d_normal, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Model**\n",
    "\n",
    "No we start training the model, taking a snapshopt for each completed block (e.g. 4x4, 8x8,...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data (909, 4, 4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data (909, 8, 8, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data (909, 16, 16, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "d:\\github\\vaporwave-gan\\vaporprogan\\venv\\lib\\site-packages\\keras\\engine\\training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# number of growth phase, e.g. 3 = 16x16 images, 6 = 128x128 images\n",
    "n_blocks = 7\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# define models\n",
    "d_models = define_discriminator(n_blocks)\n",
    "# define models\n",
    "g_models = define_generator(100, n_blocks)\n",
    "# define composite models\n",
    "gan_models = define_composite(d_models, g_models)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "# train model\n",
    "train(g_models, d_models, gan_models, dataset, latent_dim, 200, 200, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Versioning Comments\n",
    "\n",
    "V1-V9:\n",
    "\n",
    "I mainly tried out the model and upped the epoch and block size to their target values of 6 blocks (128x128 images) and 100 epochs. The outcome is very interesting but mode collapse remains a problem. So after these version I used different learning rates for the discriminator and generator.\n",
    "\n",
    "V10-12:\n",
    "\n",
    "I also added label smoothing and corrected an issue in the production of the sample images.\n",
    "\n",
    "V15:\n",
    "Introducing sigmoid function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
